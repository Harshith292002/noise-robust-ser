{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (features): (1440, 110)\n",
      "Shape of y (labels): (1440,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define a mapping for the emotions in RAVDESS\n",
    "EMOTION_MAPPING = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "def load_ravdess_data(data_path):\n",
    "    \"\"\"\n",
    "    Load the RAVDESS dataset, extracting audio features and labels.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the RAVDESS dataset folder.\n",
    "        \n",
    "    Returns:\n",
    "        X (list): List of audio features extracted from recordings.\n",
    "        y (list): List of corresponding emotion labels for each recording.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Walk through the dataset folder\n",
    "    for root, _, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                # Extract the emotion label from the file name\n",
    "                parts = file.split('-')\n",
    "                emotion_code = parts[2]\n",
    "                emotion = EMOTION_MAPPING.get(emotion_code, None)\n",
    "                \n",
    "                if emotion:\n",
    "                    # Load the audio file\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    audio, sr = librosa.load(file_path, sr=None)\n",
    "                    \n",
    "                    feature_vec = []\n",
    "                    # features explained\n",
    "                    # 0-25 each of 13 MFCC mean, std dev\n",
    "                    # 26-27 spectral centroid mean, std dev\n",
    "                    # 28-29 spectral bandwidth mean, std dev\n",
    "                    # 30-31 zero cross rate mean, std dev\n",
    "                    # 32-55 each of 12 chroma bins mean, std dev\n",
    "\n",
    "                    # Extract MFCC features\n",
    "                    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "                    mfcc_mean = np.mean(mfcc.T, axis=0)  # Take the mean of the MFCC features\n",
    "                    mfcc_std = np.std(mfcc.T, axis=0) # Take the std of the MFCC features\n",
    "                    feature_vec.extend(mfcc_mean)\n",
    "                    feature_vec.extend(mfcc_std)\n",
    "\n",
    "                    # 2. Spectral Centroid\n",
    "                    #spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "                    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "                    sc_mean = np.mean(spectral_centroid)\n",
    "                    sc_std = np.std(spectral_centroid)\n",
    "                    feature_vec.append(sc_mean)\n",
    "                    feature_vec.append(sc_std)\n",
    "                    \n",
    "                    # 3. Spectral Bandwidth\n",
    "                    #spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "                    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "                    sb_mean = np.mean(spectral_bandwidth)\n",
    "                    sb_std = np.std(spectral_bandwidth)\n",
    "                    feature_vec.append(sb_mean)\n",
    "                    feature_vec.append(sb_std)\n",
    "                    \n",
    "                    # 4. Zero-Crossing Rate\n",
    "                    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=audio)\n",
    "                    zcr_mean = np.mean(zero_crossing_rate)\n",
    "                    zcr_std = np.std(zero_crossing_rate)\n",
    "                    feature_vec.append(zcr_mean)\n",
    "                    feature_vec.append(zcr_std)\n",
    "                    \n",
    "                    # 5. Chroma Frequencies\n",
    "                    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "                    chroma_mean = np.mean(chroma.T, axis=0)\n",
    "                    chroma_std = np.std(chroma.T, axis=0)\n",
    "                    feature_vec.extend(chroma_mean)\n",
    "                    feature_vec.extend(chroma_std)\n",
    "                    \n",
    "                    '''\n",
    "                    # Extract features (MFCCs as an example)\n",
    "                    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "                    mfccs_mean = np.mean(mfccs.T, axis=0)  # Take mean across time\n",
    "                    \n",
    "                    # Append the feature and label\n",
    "                    X.append(mfccs_mean)\n",
    "                    y.append(emotion)\n",
    "                    '''\n",
    "\n",
    "                    X.append(feature_vec)\n",
    "                    y.append(emotion)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set the path to your RAVDESS dataset\n",
    "data_path = '/Users/harshith/noCloud/mlsp_project/RAVDESS'\n",
    "\n",
    "# Load data\n",
    "X, y = load_ravdess_data(data_path)\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"Shape of X (features): {X.shape}\")\n",
    "print(f\"Shape of y (labels): {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.83      0.89      0.86        38\n",
      "        calm       0.83      0.89      0.86        38\n",
      "     disgust       0.81      0.58      0.68        38\n",
      "     fearful       0.80      0.82      0.81        39\n",
      "       happy       0.78      0.74      0.76        39\n",
      "     neutral       0.58      0.58      0.58        19\n",
      "         sad       0.77      0.87      0.81        38\n",
      "   surprised       0.82      0.85      0.84        39\n",
      "\n",
      "    accuracy                           0.79       288\n",
      "   macro avg       0.78      0.78      0.78       288\n",
      "weighted avg       0.79      0.79      0.79       288\n",
      "\n",
      "Accuracy: 0.7916666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "def classify_with_svm(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate an SVM classifier using MFCC features.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: array-like, MFCC features for training\n",
    "    - y_train: array-like, corresponding emotion labels for training\n",
    "    - X_test: array-like, MFCC features for testing\n",
    "    - y_test: array-like, corresponding emotion labels for testing\n",
    "\n",
    "    Returns:\n",
    "    - None, prints classification report and accuracy\n",
    "    \"\"\"\n",
    "    # Initialize and train the SVM classifier\n",
    "    svm = SVC(kernel='rbf', C=10.0, max_iter=100000)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    # Print classification report and accuracy\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "classify_with_svm(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.neighbors import KNeighborsClassifier\\n\\n# Initialize and train the SVM classifier\\nknn = KNeighborsClassifier(n_neighbors=3)\\nknn.fit(X_train, y_train)\\n\\n# Make predictions on the test set\\ny_pred = knn.predict(X_test)\\n\\n# Print classification report and accuracy\\nprint(\"Classification Report:\")\\nprint(classification_report(y_test, y_pred))\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize and train the SVM classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Print classification report and accuracy\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def train_gmm_classifiers(features, labels):\n",
    "    \"\"\"\n",
    "    Train a GMM classifier for each emotion class.\n",
    "\n",
    "    Parameters:\n",
    "    - features: array-like, MFCC features\n",
    "    - labels: array-like, corresponding emotion labels\n",
    "\n",
    "    Returns:\n",
    "    - gmm_models: dict, a dictionary of trained GMM models for each class\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(labels)\n",
    "    gmm_models = {}\n",
    "    i = 0\n",
    "    for label in unique_labels:\n",
    "        # Select features corresponding to the current label\n",
    "        class_features = features[labels == label]\n",
    "        if i==0:\n",
    "            print(class_features.shape)\n",
    "            print(class_features[0].shape)\n",
    "            i+=1\n",
    "\n",
    "        # Train a GMM model for the current class\n",
    "        gmm = GaussianMixture(n_components=64, covariance_type='full',max_iter=10000, random_state=42)\n",
    "        gmm.fit(class_features)\n",
    "        gmm_models[label] = gmm\n",
    "\n",
    "    print(\"GMM models trained for each class.\")\n",
    "    return gmm_models\n",
    "\n",
    "def classify_with_gmm(gmm_models, features, labels):\n",
    "    \"\"\"\n",
    "    Classify features using the trained GMM models and evaluate performance.\n",
    "\n",
    "    Parameters:\n",
    "    - gmm_models: dict, a dictionary of trained GMM models\n",
    "    - features: array-like, MFCC features to classify\n",
    "    - labels: array-like, true emotion labels\n",
    "\n",
    "    Returns:\n",
    "    - None, prints classification report and accuracy\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for feature in features:\n",
    "        # Calculate the log likelihood for each class and select the class with the highest likelihood\n",
    "        log_likelihoods = {label: gmm.score([feature]) for label, gmm in gmm_models.items()}\n",
    "        predicted_label = max(log_likelihoods, key=log_likelihoods.get)\n",
    "        predictions.append(predicted_label)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(labels, predictions))\n",
    "    print(f\"Accuracy: {accuracy_score(labels, predictions)}\")\n",
    "\n",
    "# Train GMM models for each emotion class\n",
    "gmm_models = train_gmm_classifiers(X_train, y_train)\n",
    "\n",
    "# Classify and evaluate using the GMM models\n",
    "classify_with_gmm(gmm_models, X_test, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make X only MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2880\n",
      "Shape of an example MFCC matrix: (40, 366)\n",
      "Labels: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define a mapping for the emotions in RAVDESS\n",
    "EMOTION_MAPPING = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "def load_ravdess_full_mfcc(data_path, n_mfcc=40):\n",
    "    \"\"\"\n",
    "    Load the RAVDESS dataset, extracting full MFCC features and labels.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the RAVDESS dataset folder.\n",
    "        n_mfcc (int): Number of MFCC coefficients to extract.\n",
    "        \n",
    "    Returns:\n",
    "        X (list): List of full MFCC matrices (time vs. coefficients) for each recording.\n",
    "        y (list): List of corresponding emotion labels for each recording.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Walk through the dataset folder\n",
    "    for root, _, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                # Extract the emotion label from the file name\n",
    "                parts = file.split('-')\n",
    "                emotion_code = parts[2]\n",
    "                emotion = EMOTION_MAPPING.get(emotion_code, None)\n",
    "                \n",
    "                if emotion:\n",
    "                    # Load the audio file\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    audio, sr = librosa.load(file_path, sr=None)\n",
    "                    \n",
    "                    # Extract full MFCCs\n",
    "                    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "                    \n",
    "                    # Append the feature and label\n",
    "                    X.append(mfccs)\n",
    "                    y.append(emotion)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Set the path to your RAVDESS dataset\n",
    "data_path = '/Users/harshith/noCloud/mlsp_project/RAVDESS'\n",
    "\n",
    "# Load data with full MFCCs\n",
    "X, y = load_ravdess_full_mfcc(data_path)\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"Number of samples: {len(X)}\")\n",
    "print(f\"Shape of an example MFCC matrix: {X[0].shape}\")\n",
    "print(f\"Labels: {np.unique(y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Image Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def save_mfcc_images(X, y, output_dir, emotions):\n",
    "    \"\"\"\n",
    "    Save MFCC features as images for use in a DCNN.\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Array of MFCC features.\n",
    "        y (numpy.ndarray): Array of corresponding labels.\n",
    "        output_dir (str): Directory to save the images.\n",
    "        emotions (list): List of unique emotion labels.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for emotion in emotions:\n",
    "        os.makedirs(os.path.join(output_dir, emotion), exist_ok=True)\n",
    "    \n",
    "    # Determine global min and max across all MFCCs\n",
    "    global_min = min([mfcc.min() for mfcc in X])\n",
    "    global_max = max([mfcc.max() for mfcc in X])\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for i, mfcc in enumerate(X):\n",
    "        label = y[i]\n",
    "        \n",
    "        # Create a plot\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        librosa.display.specshow(mfcc, cmap='viridis', vmin=global_min, vmax=global_max)\n",
    "        #librosa.display.specshow(mfcc, x_axis='time', cmap='viridis')\n",
    "        #plt.colorbar()\n",
    "        #plt.title(f\"MFCC ({label})\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot as an image\n",
    "        file_name = f\"{i}.png\"\n",
    "        file_path = os.path.join(output_dir, label, file_name)\n",
    "        plt.savefig(file_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "# Example usage\n",
    "# Assuming `X` is a 3D array of MFCCs (e.g., shape [num_samples, n_mfcc, time_frames])\n",
    "# and `y` is a list/array of labels.\n",
    "\n",
    "output_dir = '/Users/harshith/noCloud/mlsp_project/save_images_samCode' # Directory to save images\n",
    "emotions = np.unique(y)  # Unique emotion labels in the dataset\n",
    "save_mfcc_images(X, y, output_dir, emotions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess MFCC Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Define the path to the MFCC images directory\n",
    "image_dir = '/Users/sambentlin/Documents/JHU/MLSP/Project/Code/RAVDESS_MFCC_plots_minmax_13'\n",
    "\n",
    "# Load the dataset, splitting into training and test sets\n",
    "batch_size = 32\n",
    "img_size = (128, 128)  # Resize images to a fixed size for the CNN\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    image_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    image_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Display class names (emotions)\n",
    "class_names = train_dataset.class_names\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# NORMALIZE THE DATA\n",
    "# Normalize the datasets\n",
    "normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Enable prefetching for performance\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(class_names), activation='softmax')  # One output per class\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    #validation_data=test_dataset,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# 40 epochs, yes min/max, no colorbar, 40 mfccs\n",
    "# Test Accuracy: 0.6979\n",
    "\n",
    "# 40 epochs, no min/max, yes colorbar, 40 mfccs\n",
    "# Test Accuracy: 0.6076\n",
    "\n",
    "# 40 epochs, yes min/max, no colorbar, 13 mfccs\n",
    "# Test Accuracy: 0.6215\n",
    "# Test Loss: 1.0762\n",
    "\n",
    "# 40 epochs, yes min/max, no colorbar, 13 mfccs, 2x2 window\n",
    "# Test Accuracy: 0.6736\n",
    "# Test Loss: 1.1122\n",
    "\n",
    "# 100 epochs, yes min/max, no colorbar, 40 mfccs\n",
    "# Test Accuracy: 0.7639\n",
    "\n",
    "# 100 epochs, yes min/max, no colorbar, 13 mfccs, 2x2 window\n",
    "# Test Accuracy: 0.6875\n",
    "# Test Loss: 1.5162\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Predict labels for the test dataset\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    preds = model.predict(images)\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "    y_true.extend(labels.numpy())\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(\n",
    "    y_true, y_pred, target_names=class_names\n",
    ")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=class_names, yticklabels=class_names\n",
    ")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
